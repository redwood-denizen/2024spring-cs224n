{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-01T21:18:14.128417Z",
     "start_time": "2024-08-01T21:18:14.124719Z"
    }
   },
   "outputs": [],
   "source": [
    "sents = [\n",
    "    [\n",
    "        \"the\", \"cat\", \"is\", \"a\", \"dog\"\n",
    "    ],\n",
    "    [\n",
    "        \"I\", \"love\", \"to\", \"be\", \"an\", \"engineer\"\n",
    "    ],\n",
    "    [\n",
    "        \"try\", \"me\"\n",
    "    ]\n",
    "]\n",
    "pad_token = '<pad>'\n",
    "sents_padded = []"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'is', 'a', 'dog', '<pad>']\n",
      "['I', 'love', 'to', 'be', 'an', 'engineer']\n",
      "['try', 'me', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "[['the', 'cat', 'is', 'a', 'dog', '<pad>'], ['I', 'love', 'to', 'be', 'an', 'engineer'], ['try', 'me', '<pad>', '<pad>', '<pad>', '<pad>']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences = sents.copy()\n",
    "max_length = max(len(s) for s in sentences)\n",
    "for s in sentences:\n",
    "    s.extend([pad_token] * (max_length - len(s)))\n",
    "    sents_padded.append(s)\n",
    "    print(s)\n",
    "print(sents_padded)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T21:18:28.421902Z",
     "start_time": "2024-08-01T21:18:28.417677Z"
    }
   },
   "id": "c60d2758951e098e",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:45:42.367466Z",
     "start_time": "2024-08-01T17:43:26.245840Z"
    }
   },
   "id": "dad663c6ac32f8f",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.utils\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T19:48:06.805222Z",
     "start_time": "2024-08-01T19:48:06.802520Z"
    }
   },
   "id": "84b61912ae9d8c3",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "CS224N Spring 2024: Assignment 3\n",
    "model_embeddings.py: Embeddings for the NMT model\n",
    "Pencheng Yin <pcyin@cs.cmu.edu>\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "Anand Dhoot <anandd@stanford.edu>\n",
    "Vera Lin <veralin@stanford.edu>\n",
    "Siyan Li <siyanli@stanford.edu>\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModelEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Class that converts input words to their embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, vocab):\n",
    "        \"\"\"\n",
    "        Init the Embedding layers.\n",
    "\n",
    "        @param embed_size (int): Embedding size (dimensionality)\n",
    "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
    "                              See vocab.py for documentation.\n",
    "        \"\"\"\n",
    "        super(ModelEmbeddings, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # default values\n",
    "        self.source = None\n",
    "        self.target = None\n",
    "\n",
    "        src_pad_token_idx = vocab.src['<pad>']\n",
    "        tgt_pad_token_idx = vocab.tgt['<pad>']\n",
    "\n",
    "        ### YOUR CODE HERE (~2 Lines)\n",
    "        ### TODO - Initialize the following variables:\n",
    "        ###     self.source (Embedding Layer for source language)\n",
    "        ###     self.target (Embedding Layer for target langauge)\n",
    "        ###\n",
    "        ### Note:\n",
    "        ###     1. `vocab` object contains two vocabularies:\n",
    "        ###            `vocab.src` for source\n",
    "        ###            `vocab.tgt` for target\n",
    "        ###     2. You can get the length of a specific vocabulary by running:\n",
    "        ###             `len(vocab.<specific_vocabulary>)`\n",
    "        ###     3. Remember to include the padding token for the specific vocabulary\n",
    "        ###        when creating your Embedding.\n",
    "        ###\n",
    "        ### Use the following docs to properly initialize these variables:\n",
    "        ###     Embedding Layer:\n",
    "        ###         https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "\n",
    "        self.source = nn.Embedding(len(vocab.src), embed_size, padding_idx=src_pad_token_idx)\n",
    "        self.target = nn.Embedding(len(vocab.tgt), embed_size, padding_idx=tgt_pad_token_idx)\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T20:06:44.700277Z",
     "start_time": "2024-08-01T20:06:44.693801Z"
    }
   },
   "id": "a67eeed8099d18ca",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "embed_size = 5\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T19:48:11.559953Z",
     "start_time": "2024-08-01T19:48:11.554406Z"
    }
   },
   "id": "7f899aac85a578ae",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 29\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m List\n\u001B[0;32m---> 29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m read_corpus, pad_sents\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msentencepiece\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mspm\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mVocabEntry\u001B[39;00m(\u001B[38;5;28mobject\u001B[39m):\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "CS224N Spring 2024: Assignment 3\n",
    "vocab.py: Vocabulary Generation\n",
    "Pencheng Yin <pcyin@cs.cmu.edu>\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "Vera Lin <veralin@stanford.edu>\n",
    "Siyan Li <siyanli@stanford.edu>\n",
    "\n",
    "Usage:\n",
    "    vocab.py --train-src=<file> --train-tgt=<file> [options] VOCAB_FILE\n",
    "\n",
    "Options:\n",
    "    -h --help                  Show this screen.\n",
    "    --train-src=<file>         File of training source sentences\n",
    "    --train-tgt=<file>         File of training target sentences\n",
    "    --size=<int>               vocab size [default: 50000]\n",
    "    --freq-cutoff=<int>        frequency cutoff [default: 2]\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "from docopt import docopt\n",
    "from itertools import chain\n",
    "import json\n",
    "import torch\n",
    "from typing import List\n",
    "from utils import read_corpus, pad_sents\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "class VocabEntry(object):\n",
    "    \"\"\" Vocabulary Entry, i.e. structure containing either\n",
    "    src or tgt language terms.\n",
    "    \"\"\"\n",
    "    def __init__(self, word2id=None):\n",
    "        \"\"\" Init VocabEntry Instance.\n",
    "        @param word2id (dict): dictionary mapping words 2 indices\n",
    "        \"\"\"\n",
    "        if word2id:\n",
    "            self.word2id = word2id\n",
    "        else:\n",
    "            self.word2id = dict()\n",
    "            self.word2id['<pad>'] = 0   # Pad Token\n",
    "            self.word2id['<s>'] = 1 # Start Token\n",
    "            self.word2id['</s>'] = 2    # End Token\n",
    "            self.word2id['<unk>'] = 3   # Unknown Token\n",
    "        self.unk_id = self.word2id['<unk>']\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "\n",
    "    def __getitem__(self, word):\n",
    "        \"\"\" Retrieve word's index. Return the index for the unk\n",
    "        token if the word is out of vocabulary.\n",
    "        @param word (str): word to look up.\n",
    "        @returns index (int): index of word \n",
    "        \"\"\"\n",
    "        return self.word2id.get(word, self.unk_id)\n",
    "\n",
    "    def __contains__(self, word):\n",
    "        \"\"\" Check if word is captured by VocabEntry.\n",
    "        @param word (str): word to look up\n",
    "        @returns contains (bool): whether word is contained    \n",
    "        \"\"\"\n",
    "        return word in self.word2id\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\" Raise error, if one tries to edit the VocabEntry.\n",
    "        \"\"\"\n",
    "        raise ValueError('vocabulary is readonly')\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Compute number of words in VocabEntry.\n",
    "        @returns len (int): number of words in VocabEntry\n",
    "        \"\"\"\n",
    "        return len(self.word2id)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Representation of VocabEntry to be used\n",
    "        when printing the object.\n",
    "        \"\"\"\n",
    "        return 'Vocabulary[size=%d]' % len(self)\n",
    "\n",
    "    def id2word(self, wid):\n",
    "        \"\"\" Return mapping of index to word.\n",
    "        @param wid (int): word index\n",
    "        @returns word (str): word corresponding to index\n",
    "        \"\"\"\n",
    "        return self.id2word[wid]\n",
    "\n",
    "    def add(self, word):\n",
    "        \"\"\" Add word to VocabEntry, if it is previously unseen.\n",
    "        @param word (str): word to add to VocabEntry\n",
    "        @return index (int): index that the word has been assigned\n",
    "        \"\"\"\n",
    "        if word not in self:\n",
    "            wid = self.word2id[word] = len(self)\n",
    "            self.id2word[wid] = word\n",
    "            return wid\n",
    "        else:\n",
    "            return self[word]\n",
    "\n",
    "    def words2indices(self, sents):\n",
    "        \"\"\" Convert list of words or list of sentences of words\n",
    "        into list or list of list of indices.\n",
    "        @param sents (list[str] or list[list[str]]): sentence(s) in words\n",
    "        @return word_ids (list[int] or list[list[int]]): sentence(s) in indices\n",
    "        \"\"\"\n",
    "        if type(sents[0]) == list:\n",
    "            return [[self[w] for w in s] for s in sents]\n",
    "        else:\n",
    "            return [self[w] for w in sents]\n",
    "\n",
    "    def indices2words(self, word_ids):\n",
    "        \"\"\" Convert list of indices into words.\n",
    "        @param word_ids (list[int]): list of word ids\n",
    "        @return sents (list[str]): list of words\n",
    "        \"\"\"\n",
    "        return [self.id2word[w_id] for w_id in word_ids]\n",
    "\n",
    "    def to_input_tensor(self, sents: List[List[str]], device: torch.device) -> torch.Tensor:\n",
    "        \"\"\" Convert list of sentences (words) into tensor with necessary padding for \n",
    "        shorter sentences.\n",
    "\n",
    "        @param sents (List[List[str]]): list of sentences (words)\n",
    "        @param device: device on which to load the tesnor, i.e. CPU or GPU\n",
    "\n",
    "        @returns sents_var: tensor of (max_sentence_length, batch_size)\n",
    "        \"\"\"\n",
    "        word_ids = self.words2indices(sents)\n",
    "        sents_t = pad_sents(word_ids, self['<pad>'])\n",
    "        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n",
    "        return torch.t(sents_var)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_corpus(corpus, size, freq_cutoff=2):\n",
    "        \"\"\" Given a corpus construct a Vocab Entry.\n",
    "        @param corpus (list[str]): corpus of text produced by read_corpus function\n",
    "        @param size (int): # of words in vocabulary\n",
    "        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n",
    "        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n",
    "        \"\"\"\n",
    "        vocab_entry = VocabEntry()\n",
    "        word_freq = Counter(chain(*corpus))\n",
    "        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n",
    "        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n",
    "              .format(len(word_freq), freq_cutoff, len(valid_words)))\n",
    "        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n",
    "        for word in top_k_words:\n",
    "            vocab_entry.add(word)\n",
    "        return vocab_entry\n",
    "\n",
    "    @staticmethod\n",
    "    def from_subword_list(subword_list):\n",
    "        vocab_entry = VocabEntry()\n",
    "        for subword in subword_list:\n",
    "            vocab_entry.add(subword)\n",
    "        return vocab_entry\n",
    "\n",
    "\n",
    "class Vocab(object):\n",
    "    \"\"\" Vocab encapsulating src and target langauges.\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab: VocabEntry, tgt_vocab: VocabEntry):\n",
    "        \"\"\" Init Vocab.\n",
    "        @param src_vocab (VocabEntry): VocabEntry for source language\n",
    "        @param tgt_vocab (VocabEntry): VocabEntry for target language\n",
    "        \"\"\"\n",
    "        self.src = src_vocab\n",
    "        self.tgt = tgt_vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def build(src_sents, tgt_sents) -> 'Vocab':\n",
    "        \"\"\" Build Vocabulary.\n",
    "        @param src_sents (list[str]): Source subwords provided by SentencePiece\n",
    "        @param tgt_sents (list[str]): Target subwords provided by SentencePiece\n",
    "        \"\"\"\n",
    "        # assert len(src_sents) == len(tgt_sents)\n",
    "\n",
    "        print('initialize source vocabulary ..')\n",
    "        # src = VocabEntry.from_corpus(src_sents, vocab_size, freq_cutoff)\n",
    "        src = VocabEntry.from_subword_list(src_sents)\n",
    "\n",
    "        print('initialize target vocabulary ..')\n",
    "        # tgt = VocabEntry.from_corpus(tgt_sents, vocab_size, freq_cutoff)\n",
    "        tgt = VocabEntry.from_subword_list(tgt_sents)\n",
    "\n",
    "        return Vocab(src, tgt)\n",
    "\n",
    "    def save(self, file_path):\n",
    "        \"\"\" Save Vocab to file as JSON dump.\n",
    "        @param file_path (str): file path to vocab file\n",
    "        \"\"\"\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(dict(src_word2id=self.src.word2id, tgt_word2id=self.tgt.word2id), f, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(file_path):\n",
    "        \"\"\" Load vocabulary from JSON dump.\n",
    "        @param file_path (str): file path to vocab file\n",
    "        @returns Vocab object loaded from JSON dump\n",
    "        \"\"\"\n",
    "        entry = json.load(open(file_path, 'r'))\n",
    "        src_word2id = entry['src_word2id']\n",
    "        tgt_word2id = entry['tgt_word2id']\n",
    "\n",
    "        return Vocab(VocabEntry(src_word2id), VocabEntry(tgt_word2id))\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\" Representation of Vocab to be used\n",
    "        when printing the object.\n",
    "        \"\"\"\n",
    "        return 'Vocab(source %d words, target %d words)' % (len(self.src), len(self.tgt))\n",
    "\n",
    "\n",
    "def get_vocab_list(file_path, source, vocab_size):\n",
    "    \"\"\" Use SentencePiece to tokenize and acquire list of unique subwords.\n",
    "    @param file_path (str): file path to corpus\n",
    "    @param source (str): tgt or src\n",
    "    @param vocab_size: desired vocabulary size\n",
    "    \"\"\"\n",
    "    spm.SentencePieceTrainer.Train(input=file_path, model_prefix=source, vocab_size=vocab_size)     # train the spm model\n",
    "    sp = spm.SentencePieceProcessor()   # create an instance; this saves .model and .vocab files \n",
    "    sp.Load('{}.model'.format(source))  # loads tgt.model or src.model\n",
    "    sp_list = [sp.IdToPiece(piece_id) for piece_id in range(sp.GetPieceSize())] # this is the list of subwords\n",
    "    return sp_list\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = docopt(__doc__)\n",
    "\n",
    "    print('read in source sentences: %s' % args['--train-src'])\n",
    "    print('read in target sentences: %s' % args['--train-tgt'])\n",
    "\n",
    "    src_sents = get_vocab_list(args['--train-src'], source='src', vocab_size=21000)          # EDIT: NEW VOCAB SIZE\n",
    "    tgt_sents = get_vocab_list(args['--train-tgt'], source='tgt', vocab_size=8000)\n",
    "    vocab = Vocab.build(src_sents, tgt_sents)\n",
    "    print('generated vocabulary, source %d words, target %d words' % (len(src_sents), len(tgt_sents)))\n",
    "\n",
    "    # src_sents = read_corpus(args['--train-src'], source='src')\n",
    "    # tgt_sents = read_corpus(args['--train-tgt'], source='tgt')\n",
    "\n",
    "    # vocab = Vocab.build(src_sents, tgt_sents, int(args['--size']), int(args['--freq-cutoff']))\n",
    "    # print('generated vocabulary, source %d words, target %d words' % (len(vocab.src), len(vocab.tgt)))\n",
    "\n",
    "    vocab.save(args['VOCAB_FILE'])\n",
    "    print('vocabulary saved to %s' % args['VOCAB_FILE'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T20:11:45.167734Z",
     "start_time": "2024-08-01T20:11:44.949884Z"
    }
   },
   "id": "ab1035ff528b3294",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Paul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "CS224N 2022-23: Homework 4\n",
    "utils.py: Utility Functions\n",
    "Pencheng Yin <pcyin@cs.cmu.edu>\n",
    "Sahil Chopra <schopra8@stanford.edu>\n",
    "Vera Lin <veralin@stanford.edu>\n",
    "Siyan Li <siyanli@stanford.edu>\n",
    "Moussa KB Doumbouya <moussa@stanford.edu>\n",
    "\"\"\"\n",
    "\n",
    "from typing import List\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import sentencepiece as spm\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def pad_sents(sents, pad_token):\n",
    "    \"\"\" Pad list of sentences according to the longest sentence in the batch.\n",
    "        The paddings should be at the end of each sentence.\n",
    "    @param sents (list[list[str]]): list of sentences, where each sentence\n",
    "                                    is represented as a list of words\n",
    "    @param pad_token (str): padding token\n",
    "    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter\n",
    "        than the max length sentence are padded out with the pad_token, such that\n",
    "        each sentences in the batch now has equal length.\n",
    "    \"\"\"\n",
    "    sents_padded = []\n",
    "\n",
    "    ### YOUR CODE HERE (~6 Lines)\n",
    "\n",
    "    sentences = sents.copy()\n",
    "    max_length = max(len(s) for s in sentences)\n",
    "    for s in sentences:\n",
    "        s.extend([pad_token] * (max_length - len(s)))\n",
    "        sents_padded.append(s)\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return sents_padded\n",
    "\n",
    "\n",
    "def read_corpus(file_path, source, vocab_size=2500):\n",
    "    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n",
    "    @param file_path (str): path to file containing corpus\n",
    "    @param source (str): \"tgt\" or \"src\" indicating whether text\n",
    "        is of the source language or target language\n",
    "    @param vocab_size (int): number of unique subwords in\n",
    "        vocabulary when reading and tokenizing\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load('{}.model'.format(source))\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            subword_tokens = sp.encode_as_pieces(line)\n",
    "            # only append <s> and </s> to the target sentence\n",
    "            if source == 'tgt':\n",
    "                subword_tokens = ['<s>'] + subword_tokens + ['</s>']\n",
    "            data.append(subword_tokens)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def autograder_read_corpus(file_path, source):\n",
    "    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n",
    "    @param file_path (str): path to file containing corpus\n",
    "    @param source (str): \"tgt\" or \"src\" indicating whether text\n",
    "        is of the source language or target language\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for line in open(file_path):\n",
    "        sent = nltk.word_tokenize(line)\n",
    "        # only append <s> and </s> to the target sentence\n",
    "        if source == 'tgt':\n",
    "            sent = ['<s>'] + sent + ['</s>']\n",
    "        data.append(sent)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, shuffle=False):\n",
    "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
    "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
    "    @param batch_size (int): batch size\n",
    "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
    "    \"\"\"\n",
    "    batch_num = math.ceil(len(data) / batch_size)\n",
    "    index_array = list(range(len(data)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index_array)\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        indices = index_array[i * batch_size: (i + 1) * batch_size]\n",
    "        examples = [data[idx] for idx in indices]\n",
    "\n",
    "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
    "        src_sents = [e[0] for e in examples]\n",
    "        tgt_sents = [e[1] for e in examples]\n",
    "\n",
    "        yield src_sents, tgt_sents\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-02T00:51:17.745890Z",
     "start_time": "2024-08-02T00:51:17.522132Z"
    }
   },
   "id": "fe1d2c2a33223327",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sents = [['Comencemos', 'por', 'pensar', 'en', 'los', 'pases', 'miembros', 'de', 'la', 'OCDE', ',', 'o', 'la', 'Organizacin', 'para', 'la', 'Cooperacin', 'y', 'el', 'Desarrollo', 'Econmicos', '.'], ['En', 'el', 'caso', 'de', 'control', 'de', 'armas', ',', 'realmente', 'subestimamos', 'a', 'nuestros', 'rivales', '.'], ['Sugiere', 'que', 'nos', 'interesa', 'el', 'combate', ',', 'el', 'desafo', '.'], ['Djenme', 'compartir', 'con', 'ustedes', 'aqu', 'en', 'la', 'primera', 'fila', '.'], ['Con', 'muchos', 'nmeros', '.', 'Un', 'montn']]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-02T00:51:23.872541Z",
     "start_time": "2024-08-02T00:51:23.868142Z"
    }
   },
   "id": "ee6ce5992a2bd570",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "22"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-02T00:51:43.215496Z",
     "start_time": "2024-08-02T00:51:43.210182Z"
    }
   },
   "id": "dd178853974c8247",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Comencemos', 'por', 'pensar', 'en', 'los', 'pases', 'miembros', 'de', 'la', 'OCDE', ',', 'o', 'la', 'Organizacin', 'para', 'la', 'Cooperacin', 'y', 'el', 'Desarrollo', 'Econmicos', '.'], ['En', 'el', 'caso', 'de', 'control', 'de', 'armas', ',', 'realmente', 'subestimamos', 'a', 'nuestros', 'rivales', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['Sugiere', 'que', 'nos', 'interesa', 'el', 'combate', ',', 'el', 'desafo', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['Djenme', 'compartir', 'con', 'ustedes', 'aqu', 'en', 'la', 'primera', 'fila', '.', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['Con', 'muchos', 'nmeros', '.', 'Un', 'montn', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n"
     ]
    }
   ],
   "source": [
    "print(pad_sents(sents, '<pad>'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-02T00:51:29.113627Z",
     "start_time": "2024-08-02T00:51:29.109920Z"
    }
   },
   "id": "379cf579fc8ee212",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 3, 5])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 5)\n",
    "x.size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-02T00:52:07.138854Z",
     "start_time": "2024-08-02T00:52:07.134122Z"
    }
   },
   "id": "bf5e2046fb0518c3",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([5, 2, 3])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.permute(x, (2, 0, 1)).size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-02T00:52:25.083945Z",
     "start_time": "2024-08-02T00:52:25.078691Z"
    }
   },
   "id": "6caa0af3ab4a26df",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "last_hidden = torch.tensor([[[0.1485, 0.1485],\n",
    "                      [0.1565, 0.1565],\n",
    "                      [0.1553, 0.1553],\n",
    "                      [0.1553, 0.1553],\n",
    "                      [0.1476, 0.1476]],\n",
    "\n",
    "                     [[0.1567, 0.1567],\n",
    "                      [0.1565, 0.1565],\n",
    "                      [0.1553, 0.1553],\n",
    "                      [0.1553, 0.1553],\n",
    "                      [0.1476, 0.1476]]])\n",
    "# last_hidden.shape: torch.Size([2, 5, 2])\n",
    "last_cell = torch.tensor([[[0.2753, 0.2753],\n",
    "                    [0.2882, 0.2882],\n",
    "                    [0.2860, 0.2860],\n",
    "                    [0.2860, 0.2860],\n",
    "                    [0.2714, 0.2714]],\n",
    "\n",
    "                   [[0.2886, 0.2886],\n",
    "                    [0.2882, 0.2882],\n",
    "                    [0.2860, 0.2860],\n",
    "                    [0.2860, 0.2860],\n",
    "                    [0.2714, 0.2714]]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-03T21:26:08.123072Z",
     "start_time": "2024-08-03T21:26:08.112119Z"
    }
   },
   "id": "f995c07464b213b",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.1485, 0.1485],\n        [0.1565, 0.1565],\n        [0.1553, 0.1553],\n        [0.1553, 0.1553],\n        [0.1476, 0.1476]])"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-03T21:30:21.399089Z",
     "start_time": "2024-08-03T21:30:21.393862Z"
    }
   },
   "id": "fa121cb000f0777a",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.1567, 0.1567],\n        [0.1565, 0.1565],\n        [0.1553, 0.1553],\n        [0.1553, 0.1553],\n        [0.1476, 0.1476]])"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-03T21:31:03.607600Z",
     "start_time": "2024-08-03T21:31:03.601253Z"
    }
   },
   "id": "b8332331d9e60ccc",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.1485, 0.1485, 0.1567, 0.1567],\n        [0.1565, 0.1565, 0.1565, 0.1565],\n        [0.1553, 0.1553, 0.1553, 0.1553],\n        [0.1553, 0.1553, 0.1553, 0.1553],\n        [0.1476, 0.1476, 0.1476, 0.1476]])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((last_hidden[0], last_hidden[1]), 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-03T21:33:03.075923Z",
     "start_time": "2024-08-03T21:33:03.069280Z"
    }
   },
   "id": "1aca793705d3dc80",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "Y = torch.tensor([[[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]],\n",
    "\n",
    "           [[0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500],\n",
    "            [0.1500, 0.1500, 0.1500]]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-03T23:30:31.317133Z",
     "start_time": "2024-08-03T23:30:31.278279Z"
    }
   },
   "id": "fd9fa5464094e40c",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y.shape = torch.Size([23, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(f'Y.shape = {Y.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-03T23:31:24.041690Z",
     "start_time": "2024-08-03T23:31:24.038292Z"
    }
   },
   "id": "19348b198b80b9ab",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a214604d649958a4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n",
      "Y_t.shape = torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "for Y_t in torch.split(Y,1):\n",
    "    Y_t = torch.squeeze(Y_t, dim=0)\n",
    "    print(f'Y_t.shape = {Y_t.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-03T23:39:34.869114Z",
     "start_time": "2024-08-03T23:39:34.863716Z"
    }
   },
   "id": "5715d7ff6861f558",
   "execution_count": 50
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
